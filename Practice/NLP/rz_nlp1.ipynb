{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "# from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from data_utils import Vocabulary, tokenizer\n",
    "from train_utils import train\n",
    "# import torch\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# # from glob import glob\n",
    "# from tqdm import tqdm\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "# def to_var(x, volatile=False):\n",
    "#     x = x.cuda() if torch.cuda.is_available() else x\n",
    "#     return Variable(x, volatile=volatile)\n",
    "\n",
    "\n",
    "# def detach(x):\n",
    "#     \"\"\" Detach hidden states from their history.\"\"\"\n",
    "#     return Variable(x.data) if type(x) == Variable else tuple(detach(v) for v in x)\n",
    "\n",
    "\n",
    "# # NLP = spacy.load('en')\n",
    "# NLP = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# def tokenizer(text):\n",
    "#     text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text))\n",
    "#     text = re.sub(r\"[ ]+\", \" \", text)\n",
    "#     text = re.sub(r\"\\!+\", \"!\", text)\n",
    "#     text = re.sub(r\"\\,+\", \",\", text)\n",
    "#     text = re.sub(r\"\\?+\", \"?\", text)\n",
    "#     return [x.text for x in NLP.tokenizer(text) if x.text != \" \"]\n",
    "\n",
    "\n",
    "# class Vocabulary(object):\n",
    "    \n",
    "#     def __init__(self, tokenizer):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.word2index = {}\n",
    "#         self.word2count = {}\n",
    "#         self.index2word = {}\n",
    "#         self.count = 0\n",
    "    \n",
    "#     def add_word(self, word):\n",
    "#         if not word in self.word2index:\n",
    "#             self.word2index[word] = self.count\n",
    "#             self.word2count[word] = 1\n",
    "#             self.index2word[self.count] = word\n",
    "#             self.count += 1\n",
    "#         else:\n",
    "#             self.word2count[word] += 1\n",
    "    \n",
    "#     def add_sentence(self, sentence):\n",
    "#         for word in self.tokenizer(sentence):\n",
    "#             self.add_word(word)\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         return self.count\n",
    "\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     pass\n",
    "\n",
    "\n",
    "# def train_step(model, train_dl, criterion, optimizer, scheduler):\n",
    "#     model.train()\n",
    "#     scheduler.step()\n",
    "    \n",
    "#     # init hidden states\n",
    "#     model.hidden = model.init_hidden()\n",
    "\n",
    "#     total_acc = 0.0\n",
    "#     total_loss = 0.0\n",
    "#     total = 0.0\n",
    "    \n",
    "#     for i, (train_inputs, train_labels) in tqdm(enumerate(train_dl), \n",
    "#                                                          desc='Training', \n",
    "#                                                          total=len(train_dl)):\n",
    "\n",
    "#         train_inputs, train_labels = to_var(train_inputs), to_var(train_labels)\n",
    "#         if len(train_labels) < train_dl.batch_size: continue\n",
    "        \n",
    "#         # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "#         # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "#         model.hidden = detach(model.hidden)\n",
    "#         model.zero_grad()\n",
    "#         output = model(train_inputs.t())\n",
    "        \n",
    "#         loss = criterion(output, train_labels)\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm(model.parameters(), 0.3)        \n",
    "#         optimizer.step()\n",
    "\n",
    "#         # calculate training acc and loss\n",
    "#         _, predicted = torch.max(output.data, 1)\n",
    "#         total_acc += (predicted == train_labels.data).sum()\n",
    "#         total_loss += loss.data[0]\n",
    "#         total += len(train_labels)\n",
    "        \n",
    "#     return total_loss / total, total_acc / total\n",
    "\n",
    "\n",
    "# def validate_step(model, valid_dl, criterion):\n",
    "#     model.eval()\n",
    "    \n",
    "#     # init hidden states\n",
    "#     model.hidden = model.init_hidden()\n",
    "    \n",
    "#     total_acc = 0.0\n",
    "#     total_loss = 0.0\n",
    "#     total = 0.0\n",
    "    \n",
    "#     for i, (test_inputs, test_labels) in tqdm(enumerate(valid_dl), \n",
    "#                                                        desc='Validation', \n",
    "#                                                        total=len(valid_dl)):\n",
    "\n",
    "#         test_inputs, test_labels = to_var(test_inputs, True), to_var(test_labels, True)\n",
    "#         if len(test_labels) < valid_dl.batch_size: continue\n",
    "\n",
    "#         output = model(test_inputs.t())\n",
    "#         loss = criterion(output, test_labels)\n",
    "\n",
    "#         # calculate testing acc and loss\n",
    "#         _, predicted = torch.max(output.data, 1)\n",
    "#         total_acc += (predicted == test_labels.data).sum()\n",
    "#         total_loss += loss.data[0]\n",
    "#         total += len(test_labels)\n",
    "        \n",
    "#         model.hidden = detach(model.hidden)\n",
    "        \n",
    "#     return total_loss / total, total_acc / total\n",
    "\n",
    "\n",
    "# def train(model, train_dl, valid_dl, criterion, optimizer, scheduler, num_epochs):\n",
    "#     max_len, min_count = train_dl.dataset.max_len, train_dl.dataset.min_count\n",
    "    \n",
    "#     train_hist, valid_hist = [], []\n",
    "#     best_acc, best_wts = 0.0, None\n",
    "\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "\n",
    "#         ## perform one epoch of training and validation\n",
    "#         trn_loss, trn_acc = train_step(model, train_dl, criterion, optimizer, scheduler)\n",
    "#         val_loss, val_acc = validate_step(model, valid_dl, criterion)\n",
    "\n",
    "#         train_hist += [(trn_loss, trn_acc)]\n",
    "#         valid_hist += [(val_loss, val_acc)]\n",
    "\n",
    "#         # save weights\n",
    "#         if val_acc > best_acc:\n",
    "#             best_acc = val_acc\n",
    "#             best_wts = model.state_dict().copy()\n",
    "#             torch.save(best_wts, 'models/lstm-{}-{}-{}-{}-{}-{}-{:.5f}.pth'.format(\n",
    "#                 epoch, max_len, min_count, model.embed_size, model.hidden_size, model.num_layers, best_acc))\n",
    "\n",
    "#         print('[Epoch: %3d/%3d] Training Loss: %.3f, Testing Loss: %.3f, Training Acc: %.3f, Testing Acc: %.3f'\n",
    "#               % (epoch + 1, num_epochs, trn_loss, val_loss, trn_acc, val_acc))\n",
    "\n",
    "#     model.load_state_dict(best_wts)\n",
    "#     return train_hist, valid_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup\n",
    "use_gpu = torch.cuda.is_available()\n",
    "NLP =spacy.load(\"en_core_web_sm\") #spacy.load('en')  # NLP toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Bromwell High is a cartoon comedy. \n",
    "It ran at the same time as some other programs about school life, such as 'Teachers'. \n",
    "My 35 years in the teaching profession lead me to believe that Bromwell High's \n",
    "satire is much closer to reality than is 'Teachers'. \n",
    "The scramble to survive financially, the insightful students who can see \n",
    "right through their pathetic teachers' pomp, the pettiness of the whole situation, \n",
    "all remind me of the schools I knew and their students. \n",
    "When I saw the episode in which a student repeatedly tried to burn down the school, \n",
    "I immediately recalled ......... at .......... High. \n",
    "A classic line: INSPECTOR: I'm here to sack one of your teachers. \n",
    "STUDENT: Welcome to Bromwell High. \n",
    "I expect that many adults of my age think that Bromwell High is far fetched. \n",
    "What a pity that it isn't!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy.  It ran at the same time as some other programs about school life, such as 'Teachers'.  My 35 years in the teaching profession lead me to believe that Bromwell High's  satire is much closer to reality than is 'Teachers'.  The scramble to survive financially, the insightful students who can see  right through their pathetic teachers' pomp, the pettiness of the whole situation,  all remind me of the schools I knew and their students.  When I saw the episode in which a student repeatedly tried to burn down the school,  I immediately recalled ......... at .......... High.  A classic line  INSPECTOR  I'm here to sack one of your teachers.  STUDENT  Welcome to Bromwell High.  I expect that many adults of my age think that Bromwell High is far fetched.  What a pity that it isn't!!! \n"
     ]
    }
   ],
   "source": [
    "# حذف کاراکترهای بی استفاده\n",
    "text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as 'Teachers'. My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is 'Teachers'. The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line INSPECTOR I'm here to sack one of your teachers. STUDENT Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!!! \n"
     ]
    }
   ],
   "source": [
    "# تبدیل فاصله ها به یکی\n",
    "text = re.sub(r\"[ ]+\", \" \", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as 'Teachers'. My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is 'Teachers'. The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line INSPECTOR I'm here to sack one of your teachers. STUDENT Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't! \n"
     ]
    }
   ],
   "source": [
    "# حذف علامتهای تعجب اضافی\n",
    "text = re.sub(r\"\\!+\", \"!\", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', \"'\", 'Teachers', \"'\", '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', \"'\", 'Teachers', \"'\", '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', 'INSPECTOR', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w.text for w in NLP.tokenizer(text)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and Vocabulary\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    return [x.text for x in NLP.tokenizer(text) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install SpaCy \n",
    "Installation:\n",
    "<pre>conda install -c conda-forge spacy</pre>\n",
    "\n",
    "Download a language:\n",
    "<pre>python -m spacy download en</pre>\n",
    "\n",
    "Usage:\n",
    "```python\n",
    "import spacy\n",
    "NLP = spacy.load('en')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/vahid/Documents/Personal/Machine Learning/DataSet/aclImdb_v1_rz_edit/dev'\n",
    "\n",
    "vocab_path = 'vocab.pkl'\n",
    "\n",
    "# parameters\n",
    "max_len = 200\n",
    "min_count = 10 # برای کلمات کمتر استفاده شده\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f'{data_dir}/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3004/3004 [00:00<00:00, 29426.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length = 8\n",
      "Max length = 1296\n",
      "Mean = 234.61\n",
      "Std  = 177.53\n",
      "mean + 2 * sigma = 589.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_filenames = glob(f'{data_dir}/*/*/*.txt')\n",
    "num_words = [len(open(f).read().split(' ')) for f in tqdm(all_filenames)]\n",
    "\n",
    "# print statistics\n",
    "print('Min length =', min(num_words))\n",
    "print('Max length =', max(num_words))\n",
    "\n",
    "print('Mean = {:.2f}'.format(np.mean(num_words)))\n",
    "print('Std  = {:.2f}'.format(np.std(num_words)))\n",
    "\n",
    "print('mean + 2 * sigma = {:.2f}'.format(np.mean(num_words) + 2.0 * np.std(num_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'  # special symbol we use for padding text\n",
    "UNK = '<unk>'  # special symbol we use for rare or unknown word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, tokenizer, \n",
    "                 split='train', \n",
    "                 vocab_path='vocab.pkl', \n",
    "                 max_len=100, min_count=10):\n",
    "        \n",
    "        self.path = path\n",
    "        assert split in ['train', 'test']\n",
    "        self.split = split\n",
    "        self.vocab_path = vocab_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.min_count = min_count\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.vocab = None\n",
    "        \n",
    "        self.classes = []\n",
    "        self.class_to_index = {}\n",
    "        self.text_files = []\n",
    "        \n",
    "        split_path = f'{path}/{split}'\n",
    "        for cls_idx, label in enumerate(os.listdir(split_path)):\n",
    "            text_files = [(fname, cls_idx) for fname in glob(f'{split_path}/{label}/*.txt')]\n",
    "            self.text_files += text_files\n",
    "            self.classes += [label]\n",
    "            self.class_to_index[label] = cls_idx\n",
    "        \n",
    "        self.num_classes = len(self.classes)\n",
    "            \n",
    "        # build vocabulary from training and validation texts\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # read the tokenized text file and its label (neg=0, pos=1)\n",
    "        fname, class_idx = self.text_files[index]\n",
    "        \n",
    "        if fname in self.cache:\n",
    "            return self.cache[fname], class_idx\n",
    "        \n",
    "        # read text file \n",
    "        text = open(fname).read()\n",
    "        \n",
    "        # tokenize the text file\n",
    "        tokens = self.tokenizer(text.lower())\n",
    "        \n",
    "        # padding and trimming\n",
    "        if len(tokens) < self.max_len:\n",
    "            num_pads = self.max_len - len(tokens)\n",
    "            tokens = [PAD] * num_pads + tokens\n",
    "        elif len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        # numericalizing\n",
    "        ids = torch.LongTensor(self.max_len)\n",
    "        for i, word in enumerate(tokens):\n",
    "            if word not in self.vocab.word2index:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # unknown words\n",
    "            elif word != PAD and self.vocab.word2count[word] < self.min_count:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # rare words\n",
    "            else:\n",
    "                ids[i] = self.vocab.word2index[word]\n",
    "                \n",
    "        # save in cache for future use\n",
    "        self.cache[fname] = ids\n",
    "        \n",
    "        return ids, class_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_files)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        if not os.path.exists(self.vocab_path):\n",
    "            vocab = Vocabulary(self.tokenizer)\n",
    "            filenames = glob(f'{path}/*/*/*.txt')\n",
    "            for filename in tqdm(filenames, desc='Building Vocab'):\n",
    "                with open(filename, encoding='utf8') as f:\n",
    "                    for line in f:\n",
    "                        vocab.add_sentence(line.lower())\n",
    "\n",
    "            # sort words by their frequencies\n",
    "            words = [(0, PAD), (0, UNK)]\n",
    "            words += sorted([(c, w) for w, c in vocab.word2count.items()], reverse=True)\n",
    "\n",
    "            self.vocab = Vocabulary(self.tokenizer)\n",
    "            for i, (count, word) in enumerate(words):\n",
    "                self.vocab.word2index[word] = i\n",
    "                self.vocab.word2count[word] = count\n",
    "                self.vocab.index2word[i] = word\n",
    "                self.vocab.count += 1\n",
    "\n",
    "            pickle.dump(self.vocab, open(self.vocab_path, 'wb'))\n",
    "        else:\n",
    "            self.vocab = pickle.load(open(self.vocab_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextClassificationDataset(data_dir, tokenizer, 'train', vocab_path, max_len, min_count)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_ds = TextClassificationDataset(data_dir, tokenizer, 'test', vocab_path, max_len, min_count)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2002"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0, 'pos': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "[   82     3    20     1   290   310     3    45    73     9    28   109\n",
      "   229    12   269     8  1346     6  1587  8386    15 19107     8    34\n",
      "  5136    27   148    15    55    77     6    94   938     4  1091 18864\n",
      "  4224  1091 22395    55  1404  2314    96   494    20    40  2398    27\n",
      "    86     7    56    16    89   230    12  2022     5  3440    12     2\n",
      "   520  3092   163     5    66    35   917    12     2   494     7    25\n",
      "     3   426     3   143    34     1    43   157    11     4     8  2602\n",
      "     6  2859 10929  8386     3   201     3    13   233  5017     8  3097\n",
      "    22   220   178    91   724  5466   140     5   141     8    93    48\n",
      "    28   115    11    11    16     6   455     7   635     7   135    69\n",
      "    15    92    47   566    26   563   255  1161     5  1618     4     5\n",
      "    39    15    13    48    28   391  1161     5  1618    51     2   397\n",
      "  3233    46   240    42   809     5  3264    20    15     3    97    15\n",
      "     9   620     2   354    15   100  1411    33  5383    12    75     1\n",
      "  2583     7     2    25    40  7664     8   115    12   102  4121 20065\n",
      "     7   264   888 18077     5  5103     5    42    27     5     3    53\n",
      "    11    73  1319     8   275   616     6 10207]\n"
     ]
    }
   ],
   "source": [
    "ids, label = train_ds[0]\n",
    "\n",
    "print(train_ds.classes[label])\n",
    "print(ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well , as <unk> once said , there really is n't any point in trying to pass a negative judgement that aspires to be objective on something that has had a great effect . la maman et la putain has surely passed into history as an influence on much of what 's been done in france and elsewhere in the past thirty years and no one interested in the history of film , certainly , should be <unk> from watching it . to express a purely subjective judgement , however , i feel compelled to disagree with almost every other review posted here and say to people do n't watch it it 's a waste of hours of your time that will just leave you feeling rather sick and angry . and by that i do n't mean sick and angry about the human condition or anything so general and profound as that , because that is exactly the line that most critics have adopted in their <unk> praise of the film an ordeal to watch in its ruthless dissection of our emotional cowardice and cruelty and so on and , if it really managed to put across a universally\n"
     ]
    }
   ],
   "source": [
    "# convert back the sequence of integers into original text\n",
    "print(' '.join([train_ds.vocab.index2word[int(i)] for i in ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, as Goethe once said, there really isn't any point in trying to pass a negative judgement that aspires to be objective on \"something that has had a great effect\". \"La Maman et La Putain\" has surely passed into history as an influence on much of what's been done in France and elsewhere in the past thirty years and no one interested in the history of film, certainly, should be dissuaded from watching it. To express a purely subjective judgement, however, I feel compelled to disagree with almost every other review posted here and say to people: \"Don't watch it; it's a waste of hours of your time that will just leave you feeling rather sick and angry.\" And by that I don't mean \"sick and angry\" about \"the human condition\" or anything so general and profound as that, because that is exactly the line that most critics have adopted in their fulsome praise of the film - \"an ordeal to watch in its ruthless dissection of our emotional cowardice and cruelty\" and so on - and, if it really managed to put across a universally or even broadly relevant message of this sort, then the director would have good reason to be satisfied with himself, however pessimistic his conclusions may be. My beef with the film is rather that I don't see this hours-long record of empty vanity and petty treachery as being justified or excused by any GENERALLY relevant message at all. All three main characters are deeply morally unattractive individuals: Alexandre to the greatest degree, of course, because we see by far the most of him and because he seldom shuts up for more than thirty seconds; Marie perhaps to the least degree, because we see the least of her. Alexandre's affected and pretentious monologues have a kind of amusement value, of course, but the amusement wears thin as one comes more and more clearly to realize that Jean-Pierre Léaud is most likely not even acting and that, with absurd remarks like \"un homme beau comme un film de Nicholas Ray\", he really was just reproducing word-for-word opinions that were accepted as authentic and profound by the milieu in which he, along with the director Eustache, had been living for about ten years by the time of the making of the film. I suppose if the tone of relentless superficiality and triviality had been sustained throughout 100% of the film, it might have worked as a long sardonic comedy about a particularly shallow, worthless and despicable post-'68 milieu. What made, however, this viewer at least extremely angry with the director was his granting of at least one lengthy scene each to Alexandre and Veronika in which we are clearly expected to empathize with and feel for them as if they shared a moral universe with us. If a man can get away with living in the flat of and professing to love one woman, sleeping (mostly in this very flat) with another, and running around Paris proposing marriage to yet a third, well, I suppose I can wish him the best of luck in the dog-eat-dog world he's chosen to create for himself. What I can't, however, in all conscience do is listen even for a moment to maudlin monologues from him in which he speaks about his \"anxiety\" and his \"despair\". The same goes double for the even more despicable Veronika, whom we are shown barging drunk into the apartment and even the bed shared by Marie and Alexandre and behaving there with an infantile inconsistency tantamount to the most savage and heartless cruelty. As I say, if \"La Maman et La Putain\" is intended to be nothing more nor other than a portrait of Alexandre, Veronika and Marie, three individuals whom any even halfway decent person would never admit into their company let alone their home, then I suppose there is a kind of legitimacy in praising the director for being \"unflinching\" (though why one should even feel like \"flinching\" once one had consciously opted to create such thoroughly repellent characters to filmically observe I can't imagine). The problem, however, is that the director is clearly convinced - and appears to have succeeded in convincing generations of critics - that Alexander, Veronika and Marie are somehow representative of human beings in general and of the limits of human beings' emotional capabilities. This latter idea, however, is arrant and offensive nonsense. There may indeed be an inherent fallibility and tendency to tragedy in human relations in general and sexual relations in particular. But the nature and degree of this fallibility and tendency to tragedy can only possibly be determined by people who make a sincere and serious effort to make such relations work. It surely needs no cinematic or authorial genius to convey to us the information that a man who behaves like Alexandre is going to end up hated, miserable, and alone, or that women who insist on expecting love from a man like Alexandre are going to end up disappointed and bitter. Watch \"La Maman et La Putain\" if you're historically interested in what passed for culture and human interaction in a certain post-'68 Parisian milieu which was probably, unfortunately, not restricted to just a few particularly anti-social types like these. But please don't make the mistake of believing that what is recorded here has any general relevance for humanity in the way that a film by Jean Renoir or Martin Scorsese might be argued to have.\n"
     ]
    }
   ],
   "source": [
    "# print the original text\n",
    "print(open(train_ds.text_files[0][0]).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 29512\n",
      "\n",
      "Most common words:\n",
      "the: 666690\n",
      ",: 543308\n",
      ".: 469782\n",
      "and: 324154\n",
      "a: 321797\n",
      "of: 289312\n",
      "to: 267959\n",
      "is: 217022\n",
      ">: 202239\n",
      "it: 187957\n"
     ]
    }
   ],
   "source": [
    "vocab = train_ds.vocab\n",
    "freqs = [(count, word) for (word, count) in vocab.word2count.items() if count >= min_count]\n",
    "vocab_size = len(freqs) + 2  # for PAD and UNK tokens\n",
    "print(f'Vocab size = {vocab_size}')\n",
    "\n",
    "print('\\nMost common words:')\n",
    "for c, w in sorted(freqs, reverse=True)[:10]:\n",
    "    print(f'{w}: {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Classifier\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, num_classes, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) # a lookup table\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=0.3, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(100, num_classes)\n",
    "        )\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h = to_var(torch.zeros((2*self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        c = to_var(torch.zeros((2*self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, self.hidden = self.lstm(x, self.hidden)\n",
    "        x = self.fc(x[-1])  # select the last output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29512\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 2 + len([w for (w, c) in train_ds.vocab.word2count.items() if c >= min_count])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# LSTM parameters\n",
    "embed_size = 100\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "# training parameters\n",
    "lr = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahid/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(embed_size=embed_size, \n",
    "                       hidden_size=hidden_size, \n",
    "                       vocab_size=vocab_size,\n",
    "                       num_layers=num_layers,\n",
    "                       num_classes=train_ds.num_classes, \n",
    "                       batch_size=batch_size)\n",
    "\n",
    "# if use_gpu:\n",
    "#     model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "if use_gpu:\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.7, 0.99))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahid/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Training:   0%|          | 0/41 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7e0f888e140e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/train_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, valid_dl, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m## perform one epoch of training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mtrn_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/train_utils.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, train_dl, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Starting each batch, we detach the hidden state from how it was previously produced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# If we didn't, the model would try backpropagating all the way to start of the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/utils.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\" Detach hidden states from their history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\" Detach hidden states from their history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/utils.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\" Detach hidden states from their history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\" Detach hidden states from their history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/utils.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\" Detach hidden states from their history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\" Detach hidden states from their history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/utils.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\" Detach hidden states from their history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\" Detach hidden states from their history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Personal/Machine Learning/Projects/Practice/NLP/utils.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\" Detach hidden states from their history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# map will interleave them.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "hist = train(model, train_dl, valid_dl, criterion, optimizer, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM parameters\n",
    "max_len = 400\n",
    "min_count = 10\n",
    "embed_size = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "model = LSTMClassifier(embed_size=embed_size, \n",
    "                       hidden_size=hidden_size, \n",
    "                       vocab_size=vocab_size,\n",
    "                       num_layers=num_layers,\n",
    "                       num_classes=train_ds.num_classes, \n",
    "                       batch_size=batch_size)\n",
    "\n",
    "# model.load_state_dict(torch.load('models/tmp/lstm-1-400-10-256-1024-1-0.87964.pth'))\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-3052a0f2d4dc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-3052a0f2d4dc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    word_vecs = model.embedding.weight.data =\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "word_vecs = model.embedding.weight.data =  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
